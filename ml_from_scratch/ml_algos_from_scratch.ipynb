{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note, Lasso has no closed form solution for ommiting Lasso\n",
    "\n",
    "def reg_loss(X, y, B, lmbda=0):\n",
    "    \"\"\"\n",
    "    Regression loss calculation. Suppy lambda for ridge regression.\n",
    "    Default lambda is 0 i.e no regularization\n",
    "    \"\"\"\n",
    "    return np.dot(np.transpose(y - np.dot(X, B)),y - np.dot(X, B)) + lmbda*np.dot(np.transpose(B),B)\n",
    "\n",
    "def loss_gradient(X, y, B, lmbda=0):\n",
    "    \"\"\"\n",
    "    Regression loss function gradient calculation. Suppy lambda for ridge regression.\n",
    "    Default lambda is 0 i.e no regularization\n",
    "    \"\"\"         \n",
    "    return -np.dot(np.transpose(X), y - np.dot(X, B)) + lmbda*B\n",
    "\n",
    "def log_likelihood(X, y, B,lmbda=0):\n",
    "    \"\"\"\n",
    "    Log likelihood function calculation. Suppy lambda for ridge regression.\n",
    "    Default lambda is 0 i.e no regularization\n",
    "    \"\"\"         \n",
    "    logit_res = np.dot(X, B)\n",
    "    return -np.sum( y*logit_res - np.log(1 + np.exp(logit_res)) ) - lmbda*np.dot(np.transpose(B),B)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return (1)/((1+np.exp(-z)))\n",
    "\n",
    "def log_likelihood_gradient(X, y, B, lmbda=0):\n",
    "    logit_res = np.dot(X, B)\n",
    "    preds = sigmoid(logit_res)\n",
    "    return -np.dot(np.transpose(X),(y-preds)) + lmbda*B\n",
    "\n",
    "def normalize(X):\n",
    "    \"\"\" \n",
    "    normalize x as\n",
    "    (x-mean)/std_dev\n",
    "    \"\"\"\n",
    "    # When input is dataFrame\n",
    "    if isinstance(X, pd.DataFrame): \n",
    "        for c in X.columns:\n",
    "            if is_numeric_dtype(X[c]):\n",
    "                u = np.mean(X[c])\n",
    "                std = np.std(X[c])\n",
    "                X[c] = (X[c] - u) / std\n",
    "        return\n",
    "    #when input is numpy array\n",
    "    for j in range(X.shape[1]):\n",
    "        u = np.mean(X[:,j])\n",
    "        std = np.std(X[:,j])\n",
    "        X[:,j] = (X[:,j] - u) / std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_grad_desc(X, y, loss_gradient,\n",
    "              eta=0.00001, lmbda=0.0,\n",
    "              max_iter=1000, addB0=True,\n",
    "              precision=1e-9):\n",
    "    \n",
    "    if X.ndim != 2:\n",
    "        raise ValueError(\"X must be n x p for p features\")\n",
    "    n, p = X.shape\n",
    "    if y.shape != (n, 1):\n",
    "        raise ValueError(f\"y must be n={n} x 1 not {y.shape}\")\n",
    "\n",
    "    if addB0: # for Ridge regression, set addB0 to False\n",
    "        X0 = np.ones((n,1))\n",
    "        X = np.hstack((X0, X))\n",
    "        p += 1\n",
    "\n",
    "    # initiate a random vector of Bs between [-1,1)\n",
    "    B = np.random.random_sample(size=(p, 1)) * 2 - 1\n",
    "\n",
    "    prev_B = B\n",
    "    eps = 1e-5 # To prevent division by 0\n",
    "    \n",
    "    h = 0\n",
    "    for i in range(max_iter):        \n",
    "        g = loss_gradient(X, y, prev_B, lmbda)\n",
    "        h = np.add(h, g**2)\n",
    "        if np.linalg.norm(g,2)<=precision:break #to check stopping condition\n",
    "        B = np.subtract(prev_B ,np.multiply((eta/np.sqrt(h+eps)) , g))\n",
    "        prev_B = B\n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression: \n",
    "    def __init__(self, eta=0.00001, lmbda=0.0, max_iter=1000):\n",
    "        self.eta = eta\n",
    "        self.lmbda = lmbda\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Computes the probability that the target is 1\n",
    "        \"\"\"\n",
    "        n = X.shape[0]\n",
    "        B0 = np.ones(shape=(n, 1))\n",
    "        X = np.hstack([B0, X])\n",
    "        logits_results= np.dot(X, self.B)\n",
    "        return sigmoid(logits_results)\n",
    "\n",
    "    def predict(self, X, threshold = 0.5):\n",
    "        \"\"\"\n",
    "        Computes prediction with respect to the given threshold\n",
    "        \"\"\"\n",
    "        return np.where(self.predict_proba(X)>threshold,1,0)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.B = minimize_grad_desc(X, y,log_likelihood_gradient,self.eta,self.lmbda,self.max_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self,eta=0.00001, lmbda=0.0,\n",
    "                 max_iter=1000):\n",
    "        self.eta = eta\n",
    "        self.lmbda = lmbda\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def predict(self, X):\n",
    "        n = X.shape[0]\n",
    "        B0 = np.ones(shape=(n, 1))\n",
    "        X = np.hstack([B0, X])\n",
    "        return np.dot(X, self.B)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.B = minimize_grad_desc(X, y, loss_gradient, self.eta, self.lmbda, self.max_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeRegression:\n",
    "    def __init__(self,eta=0.00001, lmbda=0.0,\n",
    "                 max_iter=1000):\n",
    "        self.eta = eta\n",
    "        self.lmbda = lmbda\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def predict(self, X):\n",
    "        n = X.shape[0]\n",
    "        B0 = np.ones(shape=(n, 1))\n",
    "        X = np.hstack([B0, X])\n",
    "        return np.dot(X, self.B)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        normalize(X)\n",
    "        self.B = np.concatenate(([[np.mean(y)]], \\\n",
    "                    minimize_grad_desc(X, y,\n",
    "                          loss_gradient,\n",
    "                          self.eta,\n",
    "                          self.lmbda,\n",
    "                          self.max_iter,addB0=False)),axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "from scipy import stats\n",
    "from sklearn.metrics import r2_score, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionNode:\n",
    "    def __init__(self, col, split, lchild, rchild):\n",
    "        self.col = col\n",
    "        self.split = split\n",
    "        self.lchild = lchild\n",
    "        self.rchild = rchild\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        if x_test[self.col]<=self.split:\n",
    "            return self.lchild.predict(x_test)\n",
    "        else:\n",
    "            return self.rchild.predict(x_test)\n",
    "    \n",
    "    def leaf(self, x_test):\n",
    "        if x_test[self.col]<=self.split:\n",
    "            return self.lchild.leaf(x_test)\n",
    "        else:\n",
    "            return self.rchild.leaf(x_test)\n",
    "\n",
    "class LeafNode:\n",
    "    def __init__(self, y, prediction):\n",
    "        self.n = len(y)\n",
    "        self.prediction = prediction\n",
    "        self.class_values = y\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        return self.prediction\n",
    "    \n",
    "    def leaf(self, x_test):\n",
    "        return self            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e24feefc12507091c1cc5c909894aa0b203b7a09f5773a65eff3ac99e8b49fe2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
